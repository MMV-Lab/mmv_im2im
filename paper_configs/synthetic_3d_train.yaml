mode: train

data:
  category: "pair"
  data_path: /mnt/eternus/users/Jianxu/projects/im2im_experiments_v1/data/synthetic3D/HIST1H2BJ/train
  dataloader:
    train:
      dataloader_params:
        batch_size: 2
        pin_memory: True
        num_workers: 4

  preprocess:
    - module_name: monai.transforms
      func_name: LoadImaged
      params:
        keys: ["IM", "GT"]
        dimension_order_out: "ZYX"
        T: 0
        C: 0
    - module_name: monai.transforms
      func_name: AddChanneld
      params:
        keys: ["IM", "GT"]          
    - module_name: monai.transforms
      func_name: ScaleIntensityRangePercentilesd
      params:
        keys: ["GT"]
        lower: 0.1
        upper: 99.9
        b_min: -1
        b_max: 1    
    - module_name: monai.transforms
      func_name: ScaleIntensityRangePercentilesd
      params:
        keys: ["IM"]
        lower: 0
        upper: 100
        b_min: -1
        b_max: 1    
    - module_name: monai.transforms
      func_name: RandSpatialCropSamplesd
      params:
        keys: ["IM", "GT"]
        random_size: False
        num_samples: 4
        roi_size: [32, 256, 256]
    - module_name: monai.transforms
      func_name: EnsureTyped
      params:
        keys: ["IM", "GT"]
  augmentation:
    - module_name: monai.transforms
      func_name: RandFlipd
      params:
        prob: 0.5
        keys: ["IM", "GT"]

model:
  framework: pix2pix
  net:
    generator:
      # type: the "type" parameter is only need for pre-defined models, like pre-defined resnet
      #type: predefined_resnet
      #params:
      #  spatial_dims: 2
      #  in_channels: 3
      #  out_channels: 3
      #  n_down_blocks: 4
      #  n_res_blocks: 9
      #  nf: 64
      #  norm_layer: "BATCH"
      #init_weight: ./lightning_logs/version_1/checkpoints/best.ckpt
      module_name: mmv_im2im.models.nets.fnet_nn_3d
      func_name: Net
      params:
        depth: 4
        mult_chan: 32
        in_channels: 1
        out_channels: 1
    #discriminator:
    #  type: predefined_basic
    #  params:
    #    spatial_dims: 2
    #    in_channels: 6
    #    n_layers: 4
    #    nf: 64
    #    norm_layer: "INSTANCE"
    discriminator:
      type: predefined_multiscale
      params:
        num_discriminator: 2
        spatial_dims: 3
        in_channels: 2
        n_layers: 3
        nf: 64
        norm_layer: "INSTANCE"
  criterion:
    loss_type: pix2pix_HD_original  # pix2pix_HD_original | pix2pix_HD  |  pix2pix_basic
    gan_loss:
      module_name: torch.nn
      func_name: BCEWithLogitsLoss  # MSELoss
      params:
        reduction: 'mean'
    # reconstruction_loss:
    #   module_name: torch.nn
    #   func_name: MSELoss
    #   params:
    #     reduction: 'mean'
    fm_loss:  # feature matching loss
      module_name: torch.nn
      func_name: L1Loss
      params:
        reduction: 'mean'
    weights:
      gan_loss: 1
      fm_loss: 10
      # reconstruction_loss: 10
  optimizer:
    generator:
      module_name: torch.optim
      func_name: AdamW  # AdamW
      params:
        lr: 0.00002
        betas: [0.5, 0.999]
        weight_decay: 0.0005
    discriminator:
      module_name: torch.optim
      func_name: Adam  # AdamW
      params:
        lr: 0.0002
        betas: [0.5, 0.999]
        weight_decay: 0.0005
  scheduler:
    generator:
      module_name: torch.optim.lr_scheduler
      func_name: ExponentialLR
      params:
        gamma: 0.999
    discriminator:
      module_name: torch.optim.lr_scheduler
      func_name: ExponentialLR
      params:
        gamma: 0.999
trainer:
  verbose: True
  params:
    accelerator: "gpu"
    devices: 1
    precision: 16
    max_epochs: 3000
    detect_anomaly: True
  # callbacks:
  #   - module_name: lightning.pytorch.callbacks.early_stopping
  #     func_name: EarlyStopping
  #     params:
  #       monitor: 'val_loss_discriminator'
  #       patience: 50 
  #   - module_name: lightning.pytorch.callbacks.model_checkpoint
  #     func_name: ModelCheckpoint
  #     params:
  #       monitor: 'val_loss_discriminator'
  #       filename: 'best'
